# FloatChat ðŸŒŠ

**An AI-Powered Conversational Interface for ARGO Ocean Data**

FloatChat is a conversational AI application that allows you to explore and analyze ARGO ocean data using natural language. Ask questions in plain English, and get back data and insights, powered by a local Large Language Model (LLM).

-----

## Features

  * **Natural Language Queries**: Ask questions like "What are the 5 deepest measurements?" or "Show me the temperature and salinity for float\_id X".
  * **Local LLM Powered**: Uses Ollama with the Phi3 model to convert your questions into SQL queries, ensuring privacy and offline capability.
  * **Interactive Frontend**: A clean and simple user interface built with Streamlit.
  * **ARGO Data Support**: Ingests and processes standard ARGO NetCDF (`.nc`) files.
  * **SQLite Backend**: Stores the processed data in a local SQLite database for fast querying.

-----

## How It Works

The application consists of three main components:

1.  **Data Processing (`main.py`)**: This script is responsible for the initial data ingestion. It reads ARGO data from a NetCDF (`.nc`) file, extracts key variables (like temperature, salinity, pressure, location, and time), and then stores the structured data into a SQLite database named `argo.db`.

2.  **AI Backend (`backend_ollama.py`)**: The core of the conversational interface. When you ask a question, this backend component:

      * Constructs a detailed prompt containing your question and the database schema.
      * Sends the prompt to a locally running Ollama LLM (`phi3:3.8b-mini-4k-instruct-q4_K_M`).
      * Receives the generated SQL query from the LLM.
      * Executes the SQL query against the `argo.db` database.
      * Returns the results as a pandas DataFrame.

3.  **Frontend (`mainStreamlit.py`)**: This script creates the user-facing web application using Streamlit. It provides a text input for you to ask questions and then displays the generated SQL query and the final data results in a clear, readable format.

-----

## Getting Started

Follow these steps to set up and run FloatChat on your local machine.

### Prerequisites

  * **Python 3.13+**
  * **Ollama**: You must have [Ollama](https://ollama.com/) installed and running.
  * **Ollama Model**: Pull the required model by running the following command in your terminal:
    ```bash
    ollama pull phi3:3.8b-mini-4k-instruct-q4_K_M
    ```

### Installation and Setup

1.  **Clone the Repository**

    ```bash
    git clone https://github.com/your-username/float-chat.git
    cd float-chat
    ```

2.  **Install Dependencies**
    The project dependencies are listed in the `pyproject.toml` file. You can install them using `pip`:

    ```bash
    pip install -r requirements.txt 
    ```

    (Note: You may need to create a `requirements.txt` file from `pyproject.toml` or install the dependencies manually)

3.  **Download ARGO Data**
    **This is a crucial step.** The application requires ARGO float data in the NetCDF (`.nc`) format.

      * You can download the data from official ARGO data sources like the [Argo Global Data Assembly Centre](https://argo.ucsd.edu/).
      * Place the downloaded `.nc` file (e.g., `20240101_prof.nc`) in the root directory of the project.

4.  **Create the Database**
    Once you have the `.nc` file, you need to process it and create the SQLite database. Run the `main.py` script:

    ```bash
    python main.py
    ```

    This will create a file named `argo.db` in the project directory.

### Running the Application

After completing the setup, you can launch the Streamlit application:

```bash
streamlit run mainStreamlit.py
```

Open your web browser and navigate to the local URL provided by Streamlit (usually `http://localhost:8501`). You can now start asking questions about the ARGO data\!

-----

## Example Questions

  * "Show the 5 deepest measurements"
  * "What is the average temperature?"
  * "List all data for float\_id '1900085'"
  * "Find the maximum salinity recorded"

-----

## Project Structure

```
float-chat/
â”œâ”€â”€ main.py               # Script to process .nc files and create the database
â”œâ”€â”€ mainStreamlit.py      # The main Streamlit application frontend
â”œâ”€â”€ backend_ollama.py     # Backend logic for LLM interaction and database querying
â”œâ”€â”€ pyproject.toml        # Project metadata and dependencies
â”œâ”€â”€ 20240101_prof.nc      # Example ARGO data file (needs to be downloaded)
â””â”€â”€ argo.db               # SQLite database (generated by main.py)
```